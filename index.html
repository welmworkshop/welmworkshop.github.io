<html><head><meta charset="UTF-8"><meta http-equiv="content-type" content="text/html;charset=UTF-8"><meta name="viewport" content="width=600, initial-scale=1"><style>body{max-width:680px;margin:auto;padding:20px;text-align:justify;line-height:140%;-webkit-font-smoothing:antialiased;-moz-osx-font-smoothing:grayscale;font-smoothing:antialiased;color:#222;font-family:Palatino,Georgia,"Times New Roman",serif}</style><style>@media print{*{-webkit-print-color-adjust:exact;text-shadow:none !important}}body{counter-reset: h1 paragraph line item list-item}@page{margin:0;size:auto}#mdContextMenu{position:absolute;background:#383838;cursor:default;border:1px solid #999;color:#fff;padding:4px 0px;font-family:-apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,Oxygen,Ubuntu,"Helvetica Neue",sans-serif;font-size:85%;font-weight:600;border-radius:4px;box-shadow:0px 3px 10px rgba(0,0,0,35%)}#mdContextMenu div{padding:0px 20px}#mdContextMenu div:hover{background:#1659d1}.md code,.md pre{font-family:Menlo,Consolas,monospace;font-size:87%;text-align:left;line-height:140%}.md .mediumToc code,.md longToc code,.md .shortToc code,.md h1 code,.md h2 code,.md h3 code,.md h4 code,.md h5 code,.md h6 code{font-size:unset}.md div.title{font-size:26px;font-weight:800;line-height:120%;text-align:center}.md div.afterTitles{height:10px}.md div.subtitle{text-align:center}.md iframe.textinsert, .md object.textinsert,.md iframe:not(.markdeep){display:block;margin-top:10px;margin-bottom:10px;width:100%;height:75vh;border:1px solid #000;border-radius:4px;background:#f5f5f4}.md .image{display:inline-block}.md img{max-width:100%;page-break-inside:avoid}.md li{text-align:left;text-indent:0}.md pre.listing {width:100%;tab-size:4;-moz-tab-size:4;-o-tab-size:4;counter-reset:line;overflow-x:auto;resize:horizontal}.md pre.listing .linenumbers span.line:before{width:30px;margin-left:-28px;font-size:80%;text-align:right;counter-increment:line;content:counter(line);display:inline-block;padding-right:13px;margin-right:8px;color:#ccc}.md div.tilde{margin:20px 0 -10px;text-align:center}.md .imagecaption,.md .tablecaption,.md .listingcaption{display:inline-block;margin:7px 5px 12px;text-align:justify;font-style:italic}.md img.pixel{image-rendering:-moz-crisp-edges;image-rendering:pixelated}.md blockquote.fancyquote{margin:25px 0 25px;text-align:left;line-height:160%}.md blockquote.fancyquote::before{content:"“";color:#DDD;font-family:Times New Roman;font-size:45px;line-height:0;margin-right:6px;vertical-align:-0.3em}.md span.fancyquote{font-size:118%;color:#777;font-style:italic}.md span.fancyquote::after{content:"”";font-style:normal;color:#DDD;font-family:Times New Roman;font-size:45px;line-height:0;margin-left:6px;vertical-align:-0.3em}.md blockquote.fancyquote .author{width:100%;margin-top:10px;display:inline-block;text-align:right}.md small{font-size:60%}.md big{font-size:150%}.md div.title,contents,.md .tocHeader,.md h1,.md h2,.md h3,.md h4,.md h5,.md h6,.md .shortTOC,.md .mediumTOC,.nonumberh1,.nonumberh2,.nonumberh3,.nonumberh4,.nonumberh5,.nonumberh6{font-family:Verdana,Helvetica,Arial,sans-serif;margin:13.4px 0 13.4px;padding:15px 0 3px;border-top:none;clear:both}.md .tocTop {display:none}.md h1,.md h2,.md h3,.md h4,.md h5,.md h6,.md .nonumberh1,.md .nonumberh2,.md .nonumberh3,.md .nonumberh4,.md .nonumberh5,.md .nonumberh6{page-break-after:avoid;break-after:avoid}.md svg.diagram{display:block;font-family:Menlo,Consolas,monospace;font-size:87%;text-align:center;stroke-linecap:round;stroke-width:2px;page-break-inside:avoid;stroke:#000;fill:#000}.md svg.diagram .opendot{fill:#fff}.md svg.diagram .shadeddot{fill:#CCC}.md svg.diagram .dotteddot{stroke:#000;stroke-dasharray:4;fill:none}.md svg.diagram text{stroke:none}@media print{@page{margin:1in 5mm;transform: scale(150%)}}@media print{.md .pagebreak{page-break-after:always;visibility:hidden}}.md a{font-family:Georgia,Palatino,'Times New Roman'}.md h1,.md .tocHeader,.md .nonumberh1{border-bottom:3px solid;font-size:20px;font-weight:bold;}.md h1,.md .nonumberh1{counter-reset:h2 h3 h4 h5 h6}.md h2,.md .nonumberh2{counter-reset:h3 h4 h5 h6;border-bottom:2px solid #999;color:#555;font-weight:bold;font-size:18px;}.md h3,.md h4,.md h5,.md h6,.md .nonumberh3,.md .nonumberh4,.md .nonumberh5,.md .nonumberh6{font-family:Verdana,Helvetica,Arial,sans-serif;color:#555;font-size:16px;}.md h3{counter-reset:h4 h5 h6}.md h4{counter-reset:h5 h6}.md h5{counter-reset:h6}.md div.table{margin:16px 0 16px 0}.md table{border-collapse:collapse;line-height:140%;page-break-inside:avoid}.md table.table{margin:auto}.md table.calendar{width:100%;margin:auto;font-size:11px;font-family:Verdana,Helvetica,Arial,sans-serif}.md table.calendar th{font-size:16px}.md .today{background:#ECF8FA}.md .calendar .parenthesized{color:#999;font-style:italic}.md table.table th{color:#FFF;background-color:#AAA;border:1px solid #888;padding:8px 15px 8px 15px}.md table.table td{padding:5px 15px 5px 15px;border:1px solid #888}.md table.table tr:nth-child(even){background:#EEE}.md pre.tilde{border-top: 1px solid #CCC;border-bottom: 1px solid #CCC;padding: 5px 0 5px 20px;margin:0 0 0 0;background:#FCFCFC;page-break-inside:avoid}.md a.target{width:0px;height:0px;visibility:hidden;font-size:0px;display:inline-block}.md a:link, .md a:visited{color:#38A;text-decoration:none}.md a:link:hover{text-decoration:underline}.md dt{font-weight:700}.md dl>dd{margin-top:-8px; margin-bottom:8px}.md dl>table{margin:35px 0 30px}.md code{page-break-inside:avoid;} @media print{.md .listing code{white-space:pre-wrap}}.md .endnote{font-size:13px;line-height:15px;padding-left:10px;text-indent:-10px}.md .bib{padding-left:80px;text-indent:-80px;text-align:left}.markdeepFooter{font-size:9px;text-align:right;padding-top:80px;color:#999}.md .mediumTOC{float:right;font-size:12px;line-height:15px;border-left:1px solid #CCC;padding-left:15px;margin:15px 0px 15px 25px}.md .mediumTOC .level1{font-weight:600}.md .longTOC .level1{font-weight:600;display:block;padding-top:12px;margin:0 0 -20px}.md .shortTOC{text-align:center;font-weight:bold;margin-top:15px;font-size:14px}.md .admonition{position:relative;margin:1em 0;padding:.4rem 1rem;border-radius:.2rem;border-left:2.5rem solid rgba(68,138,255,.4);background-color:rgba(68,138,255,.15);}.md .admonition-title{font-weight:bold;border-bottom:solid 1px rgba(68,138,255,.4);padding-bottom:4px;margin-bottom:4px;margin-left: -1rem;padding-left:1rem;margin-right:-1rem;border-color:rgba(68,138,255,.4)}.md .admonition.tip{border-left:2.5rem solid rgba(50,255,90,.4);background-color:rgba(50,255,90,.15)}.md .admonition.tip::before{content:"\24d8";font-weight:bold;font-size:150%;position:relative;top:3px;color:rgba(26,128,46,.8);left:-2.95rem;display:block;width:0;height:0}.md .admonition.tip>.admonition-title{border-color:rgba(50,255,90,.4)}.md .admonition.warn,.md .admonition.warning{border-left:2.5rem solid rgba(255,145,0,.4);background-color:rgba(255,145,0,.15)}.md .admonition.warn::before,.md .admonition.warning::before{content:"\26A0";font-weight:bold;font-size:150%;position:relative;top:2px;color:rgba(128,73,0,.8);left:-2.95rem;display:block;width:0;height:0}.md .admonition.warn>.admonition-title,.md .admonition.warning>.admonition-title{border-color:rgba(255,145,0,.4)}.md .admonition.error{border-left: 2.5rem solid rgba(255,23,68,.4);background-color:rgba(255,23,68,.15)}.md .admonition.error>.admonition-title{border-color:rgba(255,23,68,.4)}.md .admonition.error::before{content: "\2612";font-family:"Arial";font-size:200%;position:relative;color:rgba(128,12,34,.8);top:-2px;left:-3rem;display:block;width:0;height:0}.md .admonition p:last-child{margin-bottom:0}.md li.checked,.md li.unchecked{list-style:none;overflow:visible;text-indent:-1.2em}.md li.checked:before,.md li.unchecked:before{content:"\2611";display:block;float:left;width:1em;font-size:120%}.md li.unchecked:before{content:"\2610"}</style><style>.md h1::before {
content:counter(h1) " ";
counter-increment: h1;margin-right:10px}

.md h2::before {
content:counter(h1) "."counter(h2) " ";
counter-increment: h2;margin-right:10px}

.md h3::before {
content:counter(h1) "."counter(h2) "."counter(h3) " ";
counter-increment: h3;margin-right:10px}

.md h4::before {
content:counter(h1) "."counter(h2) "."counter(h3) "."counter(h4) " ";
counter-increment: h4;margin-right:10px}

.md h5::before {
content:counter(h1) "."counter(h2) "."counter(h3) "."counter(h4) "."counter(h5) " ";
counter-increment: h5;margin-right:10px}

.md h6::before {
content:counter(h1) "."counter(h2) "."counter(h3) "."counter(h4) "."counter(h5) "."counter(h6) " ";
counter-increment: h6;margin-right:10px}

</style><style>.hljs{display:block;overflow-x:auto;padding:0.5em;background:#fff;color:#000;-webkit-text-size-adjust:none}.hljs-comment{color:#006a00}.hljs-keyword{color:#02E}.hljs-literal,.nginx .hljs-title{color:#aa0d91}.method,.hljs-list .hljs-title,.hljs-tag .hljs-title,.setting .hljs-value,.hljs-winutils,.tex .hljs-command,.http .hljs-title,.hljs-request,.hljs-status,.hljs-name{color:#008}.hljs-envvar,.tex .hljs-special{color:#660}.hljs-string{color:#c41a16}.hljs-tag .hljs-value,.hljs-cdata,.hljs-filter .hljs-argument,.hljs-attr_selector,.apache .hljs-cbracket,.hljs-date,.hljs-regexp{color:#080}.hljs-sub .hljs-identifier,.hljs-pi,.hljs-tag,.hljs-tag .hljs-keyword,.hljs-decorator,.ini .hljs-title,.hljs-shebang,.hljs-prompt,.hljs-hexcolor,.hljs-rule .hljs-value,.hljs-symbol,.hljs-symbol .hljs-string,.hljs-number,.css .hljs-function,.hljs-function .hljs-title,.coffeescript .hljs-attribute{color:#A0C}.hljs-function .hljs-title{font-weight:bold;color:#000}.hljs-class .hljs-title,.smalltalk .hljs-class,.hljs-type,.hljs-typename,.hljs-tag .hljs-attribute,.hljs-doctype,.hljs-class .hljs-id,.hljs-built_in,.setting,.hljs-params,.clojure .hljs-attribute{color:#5c2699}.hljs-variable{color:#3f6e74}.css .hljs-tag,.hljs-rule .hljs-property,.hljs-pseudo,.hljs-subst{color:#000}.css .hljs-class,.css .hljs-id{color:#9b703f}.hljs-value .hljs-important{color:#ff7700;font-weight:bold}.hljs-rule .hljs-keyword{color:#c5af75}.hljs-annotation,.apache .hljs-sqbracket,.nginx .hljs-built_in{color:#9b859d}.hljs-preprocessor,.hljs-preprocessor *,.hljs-pragma{color:#643820}.tex .hljs-formula{background-color:#eee;font-style:italic}.diff .hljs-header,.hljs-chunk{color:#808080;font-weight:bold}.diff .hljs-change{background-color:#bccff9}.hljs-addition{background-color:#baeeba}.hljs-deletion{background-color:#ffc8bd}.hljs-comment .hljs-doctag{font-weight:bold}.method .hljs-id{color:#000}</style><style>div.title { padding-top: 40px; } div.afterTitles { height: 15px; }</style><style>
.md h1, .md h2 { margin-top: 0px; }
.md a:link { text-decoration: underline; }
[id^=toggle], [id^=toggle] + *{ display:none; }
[id^=toggle]:checked + *{ display:block; }
label { text-decoration: underline; font-size: 90%; }
.speakerinfo { font-size: 85%; }
</style>
</head><body style="visibility: visible;" id="md"><span class="md"><p></p><p>

<svg class="diagram" xmlns="http://www.w3.org/2000/svg" version="1.1" height="96" width="296" style="margin:0 auto 0 auto;"><g transform="translate(8,16 )">
<path d="M 8,0 L 8,48 " style="fill:none;"></path>
<path d="M 40,32 L 40,48 " style="fill:none;"></path>
<path d="M 72,0 L 72,48 " style="fill:none;"></path>
<path d="M 152,0 L 152,48 " style="fill:none;"></path>
<path d="M 208,16 L 208,64 " style="fill:none;"></path>
<path d="M 240,16 L 240,32 " style="fill:none;"></path>
<path d="M 272,16 L 272,64 " style="fill:none;"></path>
<path d="M 104,0 L 136,0 " style="fill:none;"></path>
<path d="M 104,32 L 128,32 " style="fill:none;"></path>
<path d="M 104,64 L 136,64 " style="fill:none;"></path>
<path d="M 168,64 L 192,64 " style="fill:none;"></path>
<path d="M 104,0 C 87.2,0 88,16 88,16 " style="fill:none;"></path>
<path d="M 224,0 C 207.2,0 208,16 208,16 " style="fill:none;"></path>
<path d="M 224,0 C 240.8,0 240,16 240,16 " style="fill:none;"></path>
<path d="M 256,0 C 239.2,0 240,16 240,16 " style="fill:none;"></path>
<path d="M 256,0 C 272.8,0 272,16 272,16 " style="fill:none;"></path>
<path d="M 104,32 C 87.2,32 88,48 88,48 " style="fill:none;"></path>
<path d="M 104,32 C 87.2,32 88,16 88,16 " style="fill:none;"></path>
<path d="M 24,64 C 7.199999999999999,64 8,48 8,48 " style="fill:none;"></path>
<path d="M 24,64 C 40.8,64 40,48 40,48 " style="fill:none;"></path>
<path d="M 56,64 C 39.2,64 40,48 40,48 " style="fill:none;"></path>
<path d="M 56,64 C 72.8,64 72,48 72,48 " style="fill:none;"></path>
<path d="M 104,64 C 87.2,64 88,48 88,48 " style="fill:none;"></path>
<path d="M 168,64 C 151.2,64 152,48 152,48 " style="fill:none;"></path>
<g transform="translate(0,0)"></g></g></svg>

</p><p>

</p><center>
    <strong class="asterisk"><big>Workshop on Enormous Language Models</big></strong>

<p></p><p>

    <em class="asterisk"><big>Perspectives and Benchmarks</big></em>

</p><p>

    <big>ICLR 2021</big>

</p><p>

    <a href="#speakers">Speakers</a> | <a href="#call">Call for participation</a> | <a href="#schedule">Schedule</a> | <a href="#organizers">Organizers</a>
</p></center>

<p></p><p>

</p><hr>

<p></p><p>

Language models that have been trained on unlabeled text data are a cornerstone of modern natural language processing (NLP) research, and many recent state-of-the-art results in NLP were achieved by leveraging these self-supervised models.
The success of this recipe is largely thanks to scalability: Better results can often be obtained by training larger models on larger amounts of unlabeled text data.
This synergy is particularly fruitful thanks to the wide availability of unlabeled text data on the internet and the continual improvement of hardware accelerators for training machine learning models.
Notable examples of models that make use of this scalability include <a href="https://arxiv.org/abs/1907.11692">RoBERTa</a>, which attained dramatically better performance simply by training for longer; <a href="https://arxiv.org/abs/1910.10683">T5-11B</a>, which achieved near-human performance on the challenging SuperGLUE benchmark by scaling to 11 billion parameters; <a href="https://arxiv.org/abs/2006.16668">GShard</a>, which produced a 600-billion parameter machine translation model that supports over 100 languages with state-of-the-art accuracy; and <a href="https://arxiv.org/abs/2005.14165">GPT-3</a>, which showed that scaling language models beyond 100 billion parameters could achieve strong performance on many tasks without any further task-specific training.
Indeed, the "<a href="https://arxiv.org/abs/2001.08361">scaling laws</a>" of these models demonstrate approximately log-linear improvements in performance over more than 6 orders of magnitude in parameter count.
Naïve extrapolation of these trends suggests that a model with an additional 3-5 orders of magnitude of parameters would saturate performance on most current benchmarks.

</p><p>

These results place our field at a crossroads.
Will scaling lead to models that outperform humans on all text-based tasks, or are there limits to the scalability of these models?
Should we focus on simply scaling these models, or should we design more sophisticated architectures and training schemes?
Do our current benchmark effectively test capabilities that humans can master but large language models lack?
How can we address the legal and ethical issues that arise from using unstructured web crawls for training language models?
What can we learn from the fields of cognition, linguistics, and philosophy as we attempt to measure the â€œintelligenceâ€ of machines?
The goal of this workshop is to find answers to these questions by inviting a diverse group of researchers to critically examine the state of giant language models.
We also hope to provide concrete evidence of the capabilities and limitations of current enormous language models through a participant-driven benchmark.

</p>
<div class="nonumberh1"><a id="speakers">Confirmed speakers and panelists</a></div>
<p>

<table width="100%"><tbody><tr valign="top"><td>

<p></p><p>

</p><center><div class="image" style=""><a href="https://faculty.washington.edu/ebender/" target="_blank"><img class="markdeep" src="images/emilybender.jpg" width="150" height="150"></a><center><span class="imagecaption">Emily M. Bender</span></center></div></center>

<p></p><p>

</p></td><td>

<p></p><p>

 </p><center><div class="image" style=""><a href="https://sites.google.com/uw.edu/angelinamcmillan-major/home" target="_blank"><img class="markdeep" src="images/angelinamcmillanmajor.jpg" width="150" height="150"></a><center><span class="imagecaption">Angelina <br> McMillan-Major</span></center></div></center>

<p></p><p>

</p></td><td>

<p></p><p>

 </p><center><div class="image" style=""><a href="https://www.linkedin.com/in/noam-shazeer-3b27288" target="_blank"><img class="markdeep" src="images/noamshazeer.jpg" width="150" height="150"></a><center><span class="imagecaption">Noam Shazeer</span></center></div></center>

<p></p><p>

</p></td><td>

<p></p><p>

 </p><center><div class="image" style=""><a href="https://homes.cs.washington.edu/~yejin/" target="_blank"><img class="markdeep" src="images/yejinchoi.jpg" width="150" height="150"></a><center><span class="imagecaption">Yejin Choi</span></center></div></center>

<p></p><p>

</p></td></tr><tr valign="top"><td>

<p></p><p>

</p><center><div class="image" style=""><a href="https://thomwolf.io/" target="_blank"><img class="markdeep" src="images/thomaswolf.jpg" width="150" height="150"></a><center><span class="imagecaption">Thomas Wolf</span></center></div></center>

<p></p><p>

</p></td><td>

<p></p><p>

 </p><center><div class="image" style=""><a href="https://natschluter.github.io/" target="_blank"><img class="markdeep" src="images/natalieschluter.jpg" width="150" height="150"></a><center><span class="imagecaption">Natalie Schluter</span></center></div></center>

<p></p><p>

</p></td><td>

<p></p><p>

 </p><center><div class="image" style=""><a href="https://psychology.berkeley.edu/people/alison-gopnik" target="_blank"><img class="markdeep" src="images/alisongopnik.jpg" width="150" height="150"></a><center><span class="imagecaption">Alison Gopnik</span></center></div></center>

<p></p><p>

</p></td><td>

<p></p><p>

 </p><center><div class="image" style=""><a href="https://www.linkedin.com/in/emily-d-4553b9194" target="_blank"><img class="markdeep" src="images/emilydinan.jpg" width="150" height="150"></a><center><span class="imagecaption">Emily Dinan</span></center></div></center>

<p></p><p>

</p></td></tr><tr valign="top"><td>

<p></p><p>

</p><center><div class="image" style=""><a href="https://nicholas.carlini.com/" target="_blank"><img class="markdeep" src="images/nicholascarlini.jpg" width="150" height="150"></a><center><span class="imagecaption">Nicholas Carlini</span></center></div></center>

<p></p><p>

</p></td><td>

<p></p><p>

 </p><center><div class="image" style=""><a href="http://www.cs.cmu.edu/~jessed/" target="_blank"><img class="markdeep" src="images/jessedodge.jpg" width="150" height="150"></a><center><span class="imagecaption">Jesse Dodge</span></center></div></center>

<p></p><p>

</p></td><td>

<p></p><p>

 </p><center><div class="image" style=""><a href="https://www.law.kuleuven.be/citip/en/staff-members/staff/00137042" target="_blank"><img class="markdeep" src="images/thomasmargoni.jpg" width="150" height="150"></a><center><span class="imagecaption">Thomas Margoni</span></center></div></center>

<p></p><p>

</p></td><td>

<p></p><p>

 </p><center><div class="image" style=""><a href="https://research.fb.com/people/lewis-mike/" target="_blank"><img class="markdeep" src="images/mikelewis.jpg" width="150" height="150"></a><center><span class="imagecaption">Mike Lewis</span></center></div></center>

<p></p><p>

</p></td></tr></tbody></table>

</p>
<div class="nonumberh1"><a id="call">Call for participation</a></div>
<p>

This workshop will have a non-standard submission format: Rather than submitting research papers, participants will be invited to contribute diverse tasks that they believe measure uniquely human or particularly challenging capabilities for large language models.
Teams at Google and OpenAI have committed to evaluate this task set on their best-performing model architectures, across models spanning from tens of thousands through hundreds of billions or more of parameters.
Researchers will also be invited to contribute and evaluate their own models on these tasks.
We will analyze these experiments, and report the results at the workshop, with a particular focus on how model performance on different task types scales with model size.
By inviting contributions of tasks or models, we provide a means for researchers to participate whether or not they have the (cost-prohibitive) computational resources to train giant language models.
The end result will be the <em class="asterisk">Beyond the Imitation Game Benchmark (BIG-bench)</em>: A novel participant-driven test of the limits of giant language models.

</p><p>

<strong class="asterisk">Accepted task authors will be invited to be co-authors on the paper announcing the benchmark.</strong> BIG-bench task submissions will be accepted until <strong class="asterisk">June 1, 2021</strong>, after the workshop. Find out more about BIG-bench and participate <a href="https://github.com/google/BIG-Bench">here</a>.

</p>
<div class="nonumberh1"><a id="schedule">Schedule</a></div>
<p>

Like all ICLR 2021 workshops, WELM will be held remotely on <strong class="asterisk">Friday, May 7th 2021</strong>.
All times listed below are in the <a href="https://www.timeanddate.com/worldclock/converter.html?iso=20210507T144500&p1=3920">UTC-06:00 timezone</a> ("US Mountain Time").
You can view a Google Calendar of all of the events <a href="https://calendar.google.com/calendar/u/0/embed?src=rc5a70prorns7q5ergagn0rd5g@group.calendar.google.com&ctz=America/Denver">here</a>.
</p><div class="table"><table class="table"><tbody><tr><th style="text-align:left"> Time </th><th style="text-align:left"> Event </th></tr>
<tr><td style="text-align:left"> 8:45-9:00am </td><td style="text-align:left"> Opening remarks </td></tr>
<tr>
<td style="text-align:left"> 9:00-9:30am </td><td style="text-align:left"> Invited talk: "<i>Brief copyright reflections on enormous language model training</i>" by Thomas Margoni (<label for="toggle-margoni">details</label>)<input id="toggle-margoni" type="checkbox">
<div class="speakerinfo">
<strong class="asterisk">Abstract</strong>: Is the use of “data” available on the Internet for the purpose of training language models lawful, or should prior authorisation be obtained? This apparently simple question reveals the complexity of a field intersecting law, technology and the usually borderless nature of the Internet. In this talk we will focus on copyright’s international framework, including a few comparative references (mainly EU and US) to offer contextual examples. A selection of some of the most “popular” aspects will be briefly addressed, in particular taxonomies (“data” for NLP is not “data” for copyright law), rights (how many and what types of copies are made), and licenses (should only Creative Commons works be used?). Finally, some concluding remarks on the role of legal rules in favouring open, fair and accountable technological developments will be formulated.<br /><br />
<strong class="asterisk">Bio</strong>: Thomas Margoni is Research Professor of Intellectual Property Law and a member of the Borad of Directors of the Centre for IT &amp; IP Law (CiTiP), Faculty of Law, KU Leuven (BE). His work concentrates on international, comparative and EU copyright law applied to new technologies and he is an expert on <a href="http://eprints.gla.ac.uk/159231/13/159231.pdf">legal issues</a> pertaining to training language models.
</div></td></tr>
<tr><td style="text-align:left"> 9:30-10:00am </td><td style="text-align:left"> Invited talk: "<i>What information to report about our data</i>" by Jesse Dodge (<label for="toggle-dodge">details</label>)<input id="toggle-dodge" type="checkbox">
<div class="speakerinfo">
<strong class="asterisk">Abstract</strong>: Natural language processing and machine learning have grown tremendously in recent years, and researchers hold myriad opinions on what to report in their papers. In this talk I will present a high-level overview of the NLP Reproducibility Checklist, which provides general recommendations for what information to report in NLP papers. Then, I will dive into an example of documenting C4, a massive unlabeled text corpus built from web-crawled data. Finally, I will introduce a framework for modeling bias in data, show that this framework recovers annotation artifacts in existing datasets, and describe a technique which can help mitigate the impact of such artifacts.<br /><br />
<strong class="asterisk">Bio</strong>: Jesse Dodge is a postdoctoral researcher at AllenAI who recently completed a PhD in Computer Science from Carnegie Mellon University. He has done extensive work into the <a href="https://arxiv.org/abs/2002.06305">reproducibility</a> and <a href="https://arxiv.org/abs/1909.03004">reporting</a> of research on giant language models, as well as the implications of their <a href="https://arxiv.org/abs/1907.10597">energy and financial cost</a>.
</div></td></tr>
<tr><td style="text-align:left"> 10:00-10:30am </td><td style="text-align:left"> Invited talk: "<i>On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? 🦜</i>" by Emily M. Bender and Angelina McMillan-Major (<label for="toggle-bender-mcmillan-major">details</label>)<input id="toggle-bender-mcmillan-major" type="checkbox">
<div class="speakerinfo">
<strong class="asterisk">Abstract</strong>: (Joint work with Timnit Gebru and Margaret Mitchell) The past 3 years of work in NLP have been characterized by the development and deployment of ever larger language models, especially for English. BERT, its variants, GPT-2/3, and others, most recently Switch-C, have pushed the boundaries of the possible both through architectural innovations and through sheer size. Using these pretrained models and the methodology of fine-tuning them for specific tasks, researchers have extended the state of the art on a wide array of tasks as measured by leaderboards on specific benchmarks for English. In this paper, we take a step back and ask: How big is too big? What are the possible risks associated with this technology and what paths are available for mitigating those risks? We provide recommendations including weighing the environmental and financial costs first, investing resources into curating and carefully documenting datasets rather than ingesting everything on the web, carrying out pre-development exercises evaluating how the planned approach fits into research and development goals and supports stakeholder values, and encouraging research directions beyond ever larger language models.<br /><br />
<strong class="asterisk">Bio</strong>: Emily M. Bender is a Professor of Linguistics at the University of Washington. Her research is focused on multilingual grammar engineering, the study of variation, both within and across languages, the relationship between linguistics and computational linguistics, and practical methods for promoting engagement with ethical issues in NLP. She coined the <a href="https://thegradient.pub/the-benderrule-on-naming-the-languages-we-study-and-why-it-matters/">Bender Rule</a>, co-created <a href="https://www.mitpressjournals.org/doi/pdf/10.1162/tacl_a_00041">Data Statements</a>, and is a co-author of the recent <a href="https://dl.acm.org/doi/abs/10.1145/3442188.3445922">Stochastic Parrots 🦜</a> paper.<br>Angelina McMillan-Major is a PhD student in Computational Linguistics at the University of Washington. She is interested in methodologies for low-resource language documentation and revitalization, including machine learning methodologies, and thinking critically about the interaction between technology and language. She is a co-author of the recent <a href="https://dl.acm.org/doi/abs/10.1145/3442188.3445922">Stochastic Parrots 🦜</a> paper.
</div></td></tr>
<tr><td style="text-align:left"> 10:30-10:45am </td><td style="text-align:left"> Break to discuss talks and questions for panel #1 </td></tr>
<tr><td style="text-align:left"> 10:45-11:15am </td><td style="text-align:left"> Invited talk: "<i>BigScience: building a Large-Hadron-Collider in AI and NLP</i>" by Thomas Wolf (<label for="toggle-wolf">details</label>)<input id="toggle-wolf" type="checkbox">
<div class="speakerinfo">
<strong class="asterisk">Abstract:</strong> The acceleration in Artificial Intelligence (AI) and Natural Language Processing (NLP) will have a fundamental impact on society, as these technologies are at the core of the tools we use on a daily basis. A considerable part of this effort currently stems in NLP from training increasingly larger language models on increasingly larger quantities of texts. Unfortunately, the resources necessary to create the best-performing models are found mainly in industry rather than academia. This unbalance on a transformative technology poses problems, from a research advancement, environmental, ethical and societal perspective. The BigScience project aims to demonstrate another way of creating, studying, and sharing large language models and large research artifacts in general within the AI/NLP research communities. BigScience takes inspiration from scientific creation schemes existing in other scientific fields, such as CERN and the LHC in particle physics, in which open scientific collaborations facilitate the creation of large-scale artifacts useful for the entire research community. Gathering a much larger research community around the creation of these artifacts makes it possible to consider in advance the many research questions surrounding large language models (capabilities, limitations, potential improvements, bias, ethics, environmental impact, general AI/cognitive research landscape) that will be interesting to answer with the created artifacts and to reflect and prepare the tools needed to answer as many of these questions as possible. The BigScience project is seen as a proposal for an alternative way to conduct large scale science projects in a more international and inclusive way. Beyond the research artifacts created and shared, the project’s success will ultimately be measured by its long-term impact on the field: by proposing another way for large-scale collaborations inspired by the successes in fields like particle physics.<br /><br />
<strong class="asterisk">Bio:</strong> Thomas Wolf is co-founder and Chief Science Officer of HuggingFace. His team is on a mission to catalyze and democratize NLP research. Prior to HuggingFace, Thomas gained a Ph.D. in physics, and later a law degree. He worked as a physics researcher and a European Patent Attorney.</div>
</td></tr>
<tr><td style="text-align:left"> 11:15-11:45am </td><td style="text-align:left"> Invited talk: "<i>Adversarial Benchmarking for Toxic Generation in Large Language Models</i>" by Emily Dinan (<label for="toggle-dinan">details</label>)<input id="toggle-dinan" type="checkbox">
<div class="speakerinfo">
<strong class="asterisk">Abstract:</strong> Large language models trained on massive unlabeled corpora of human interactions will learn patterns and mimic behaviors therein, which often include offensive or otherwise toxic behavior. In this talk, I will discuss the problem of toxic generation for large language models. We will explore why this problem is challenging from both a technical and ethical perspective. I will highlight adversarial benchmarking — with humans and models in the loop — as one possible path for making progress on these issues, among many others. Lastly, I will discuss open problems and next steps for this line of work.<br /><br />
<strong class="asterisk">Bio:</strong> Emily Dinan is a research engineer at Facebook AI. She works mainly on dialogue systems and adversarial benchmarks to better measure their capabilities. Past work along these lines include the <a href="https://arxiv.org/abs/1908.06083">Build it Break it Fix it</a> and <a href="https://arxiv.org/abs/1910.14599">Adversarial NLI</a> benchmarks, research into <a href="https://arxiv.org/abs/1911.03842">bias</a> and <a href="https://arxiv.org/abs/2010.07079">other harmful behaviors</a> in dialogue models, and <a href="https://arxiv.org/abs/2004.13637">open-source efforts to build large scale open-domain chatbots</a>.
</div> </td></tr>
<tr><td style="text-align:left"> 11:45-12:00pm </td><td style="text-align:left"> Break to discuss talks and questions for panel #1 </td></tr>
<tr><td style="text-align:left"> 12:00-12:45pm </td><td style="text-align:left"> Panel #1: “Bias, safety, copyright, and efficiency” with Thomas Wolf, Thomas Margoni, Emily Dinan, Natalie Schluter, and Jesse Dodge</td></tr>
<tr><td style="text-align:left"> 12:45-1:07pm </td><td style="text-align:left"> BIG-bench introduction and initial results by Jascha Sohl-Dickstein </td></tr>
<tr><td style="text-align:left"> 1:07-1:27pm </td><td style="text-align:left"> BIG-bench spotlight talks (two minutes each):<br>
<ol>
    <li><a href="https://github.com/google/BIG-bench/tree/main/bigbench/benchmark_tasks/winowhy_json_multiple_choice/">WinoWhy</a> by Hongming Zhang, Xinran Zhao</li>
    <li><a href="https://github.com/google/BIG-bench/tree/main/bigbench/benchmark_tasks/symbol_interpretation_task/">Symbol Interpretation Task (SIT)</a> by Antonio Norelli, <i>et al.</i></li>
    <li><a href="https://github.com/google/BIG-bench/tree/main/bigbench/benchmark_tasks/logic_grid_puzzle_task/">Logic Grid Puzzles</a> by Jeremy Kim, <i>et al.</i></li>
    <li><a href="https://github.com/google/BIG-bench/tree/main/bigbench/benchmark_tasks/word_problems_on_sets_and_graphs/">Word problems on sets and graphs</a> by Benjamin Inden</li>
    <li><a href="https://github.com/google/BIG-bench/tree/main/bigbench/benchmark_tasks/operators/">Operators</a> by Jos Rozen</li>
    <li><a href="https://github.com/google/BIG-bench/tree/main/bigbench/benchmark_tasks/gem/">GEM</a> by Sebastian Gehrmann, <i>et al.</i></li>
    <li><a href="https://github.com/google/BIG-bench/tree/main/bigbench/benchmark_tasks/logical_deduction/">Logical Deduction</a> by James Simon, Chandan Singh</li>
    <li><a href="https://github.com/google/BIG-bench/tree/main/bigbench/benchmark_tasks/cryptonite/">Cryptonite</a> by Avia Efrat, <i>et al.</i></li>
    <li><a href="https://github.com/google/BIG-bench/tree/main/bigbench/benchmark_tasks/formal_fallacies_syllogisms_negation/">Formal Fallacies and Syllogisms</a> by Gregor Betz, <i>et al.</i></li>
    <li><a href="https://github.com/google/BIG-bench/tree/main/bigbench/benchmark_tasks/goal_step_wikihow/">Goal Step Inference</a> by Li Zhang, <i>et al.</i></li>
</ol>
    </td></tr>
<tr><td style="text-align:left"> 1:27-2:00pm </td><td style="text-align:left"> BIG-bench contributed talks (ten minutes each):<br>
<ol>
    <li><a href="https://github.com/google/BIG-bench/tree/main/bigbench/benchmark_tasks/strategyqa/">Strategy QA</a> by Mor Geva, <i>et al.</i></li>
    <li><a href="https://github.com/google/BIG-bench/tree/main/bigbench/benchmark_tasks/gender_sensitivity_test_English/">Gender Sensitivity Test English</a> and <a href="https://github.com/google/BIG-bench/tree/main/bigbench/benchmark_tasks/gender_sensitivity_test_Chinese/">Gender Sensitivity Test Chinese</a> by Xudong Shen</li>
    <li>Joint talk: <a href="https://github.com/google/BIG-bench/tree/main/bigbench/benchmark_tasks/linguistics_puzzles_json/">Linguistics Puzzles</a> by Nathan A. Chi & <a href="https://github.com/google/BIG-bench/tree/main/bigbench/benchmark_tasks/conlang_translation/">Conlang Translation Problems</a> by Rowan Jacobs, <i>et al.</i></li>
</ol>
    </td></tr>
    <tr><td style="text-align:left"> 2:00-2:30pm </td><td style="text-align:left"> Invited talk: "<i>Noam's Neural Network Notation for Distributed Deep Learning</i>" by Noam Shazeer (<label for="toggle-shazeer">details</label>)<input id="toggle-shazeer" type="checkbox">
<div class="speakerinfo">
<strong class="asterisk">Abstract</strong>: Training enormous language models requires innovative distributed computation algorithms.  I will cover the common algorithms, and present a partially novel notation for describing them. <br /><br />
<strong class="asterisk">Bio</strong>: Noam Shazeer is a principal software engineer at Google Brain. He has worked on large neural language models for many years, starting with work on training <a href="https://arxiv.org/abs/1602.02410">giant recurrent neural network LMs</a>, developing the <a href="https://arxiv.org/abs/1701.06538">Mixture-of-Experts layer</a> to train 100+ billion parameter models; designing the <a href="https://arxiv.org/abs/1706.03762">Transformer architecture</a>, building the <a href="https://arxiv.org/abs/1811.02084">Mesh-TensorFlow</a> and <a href="https://arxiv.org/abs/2006.16668">GShard</a> libraries, and releasing the pre-trained <a href="https://arxiv.org/abs/1910.10683">T5</a> model.
</div></td></tr>
<tr><td style="text-align:left"> 2:30-3:00pm </td><td style="text-align:left"> Invited talk: "<i>Beyond Brute Force Scaling</i>" by Mike Lewis (<label for="toggle-lewis">details</label>)<input id="toggle-lewis" type="checkbox">
<div class="speakerinfo">
<strong class="asterisk">Abstract</strong>: Remarkable results have been achieved by using ever more compute to train even larger transformer language models. Is further progress possible without increasing the computational cost? I will focus on two promising alternative paradigms for more efficiently scaling up the capacity of language models. Firstly, I will discuss non-parametric language models, which use explicit memorization over large amounts of text. In particular, kNN-LM uses distances between representations from a pre-trained language in a nearest neighbour classifier, which can dramatically improve perplexity with no additional training. Secondly, I will describe recent work on sparse models, where only a small subset of parameters are used on any given training example. I will introduce BASE layers, which provide the first “drop in” expert layer that can be used without modifying the model training objective. I will also speculate about why our models need to be so big, and future directions for moving beyond their limitations.<br /><br />
<strong class="asterisk">Bio</strong>: Mike Lewis is a research scientist at Facebook AI. He has worked on a diverse set of large language models, including the <a href="https://arxiv.org/abs/1907.11692">RoBERTa</a> model that showed the importance of large-scale pre-training, the <a href="https://arxiv.org/abs/1910.13461">BART</a> sequence-to-sequence model and its multilingual counterpart <a href="https://arxiv.org/abs/2001.08210">mBART</a>, and the <a href="https://arxiv.org/abs/2006.15020">MARGE</a>, <a href="https://arxiv.org/abs/2005.11401">RAG</a>, and <a href="https://openreview.net/forum?id=HklBjCEKvH">k-NN LM</a> architectures that make use of a nonparametric memory.
</div></td></tr>
<tr><td style="text-align:left"> 3:00-3:30pm </td><td style="text-align:left"> Invited talk: "<i>Privacy & Enormous Language Models</i>" by Nicholas Carlini (<label for="toggle-carlini">details</label>)<input id="toggle-carlini" type="checkbox">
<div class="speakerinfo">
<strong class="asterisk">Abstract</strong>: This talk studies the privacy implications of training enormous language models on private datasets. Given access to GPT-2, we show it is possible to extract individual examples that were used to train the model. For example, we recover the full name, address, phone number, and email address of an individual person who happened to have their information in the training dataset. Most worryingly, we find that larger (billion parameter) models memorize significantly more information than smaller (hundred million parameter) models. As models continue to scale to larger sizes and larger datasets, it will be necessary to carefully understand their propensity to memorization. Since preventing these attacks is likely to require significant advancements in private training techniques, we argue that empirically measuring the privacy of language models (or lack thereof) is an important component of the release process.<br /><br />
<strong class="asterisk">Bio</strong>: Nicholas Carlini is a research scientist at Google Brain. He studies the security and privacy of machine learning, for which he has received best paper awards at ICML and IEEE S&amp;P. He obtained his PhD from the University of California, Berkeley in 2018.
</div></td></tr>
<tr><td style="text-align:left"> 3:30-3:45pm </td><td style="text-align:left"> Break to discuss talks and questions for panel #2 </td></tr>
<tr><td style="text-align:left"> 3:45-4:15pm </td><td style="text-align:left"> Invited talk: "<i>Causal, counterfactual and relational inference: Can a language model be as smart as a toddler?</i>" by Alison Gopnik (<label for="toggle-gopnik">details</label>)<input id="toggle-gopnik" type="checkbox">
<div class="speakerinfo">
<strong class="asterisk">Abstract</strong>: I will outline recent theoretical and empirical work which shows that 2-4 year old children can make causal and counterfactual inferences, extrapolate causal functions and perform analogical reasoning in new settings with novel objects, causal systems and variables.. This allows them to draw dramatically new conclusions from small amounts of evidence. Models of human thought should be able to make similar inferences. An interesting possibility is that these capacities are related to the development of language. <br /><br />
<strong class="asterisk">Bio</strong>: Alison Gopnik is a Professor of Psychology, Affiliate Professor of Philosophy and member of the Berkeley Artificial Intelligence Research Group. Her research explores how young children come to know about the world around them. In particular, she researches how children build causal structure from patterns of data across physical, biological, and psychological domains.
</div></td></tr>
<tr><td style="text-align:left"> 4:15-4:45pm </td><td style="text-align:left"> Invited talk: "<i>David V.S. Goliath in the Era of Gigantic Neural Networks</i>" by Yejin Choi (<label for="toggle-choi">details</label>)<input id="toggle-choi" type="checkbox">
<div class="speakerinfo">
<strong class="asterisk">Abstract</strong>: In this talk, I'll discuss the art of battling giants, where we might find the underdogs, and what could possibly go wrong with GPT-3. <br /><br />
<strong class="asterisk">Bio</strong>: Yejin Choi is an associate professor of Computer Science &amp; Engineering at the University of Washington with the Brett Helsel Career Development Professorship, adjunct of the Linguistics department, and affiliate of the Center for Statistics and Social Sciences. Together with her students, she developed the <a href="https://arxiv.org/abs/1905.12616">GROVER</a> model for generating and detecting fake news, the <a href="https://leaderboard.allenai.org/hellaswag/submission/bsd2v81mvoqvi4gaj6tg">UNICORN</a> multi-task model, and many difficult benchmarks for giant language models such as <a href="https://arxiv.org/abs/1911.03705">CommonGen</a> and <a href="https://arxiv.org/abs/2004.03607">TuringAdvice</a>.
</div></td></tr>
<tr><td style="text-align:left"> 4:45-5:00pm </td><td style="text-align:left"> Break to discuss talks and questions for panel #2 </td></tr>
<tr><td style="text-align:left"> 5:00-5:45pm </td><td style="text-align:left"> Panel #2: “Extrapolating the capabilities of language models” with Alison Gopnik, Yejin Choi, Mike Lewis, and Emily M. Bender</td></tr>
<tr><td style="text-align:left"> 5:45-6:00pm </td><td style="text-align:left"> Closing remarks </td></tr>
</tbody></table></div>

<p></p>
<div class="nonumberh1"><a id="organizers">Organizers</a></div>
<p>

<table width="100%"><tbody><tr valign="top"><td>

<p></p><p>

</p><center><div class="image" style=""><a href="http://colinraffel.com/" target="_blank"><img class="markdeep" src="images/colinraffel.jpg" width="150" height="150"></a><center><span class="imagecaption">Colin Raffel</span></center></div></center>

<p></p><p>

</p></td><td>

<p></p><p>

 </p><center><div class="image" style=""><a href="https://research.google/people/104881/" target="_blank"><img class="markdeep" src="images/adamroberts.jpg" width="150" height="150"></a><center><span class="imagecaption">Adam Roberts</span></center></div></center>

<p></p><p>

</p></td><td>

<p></p><p>

 </p><center><div class="image" style=""><a href="https://askell.io/" target="_blank"><img class="markdeep" src="images/amandaaskell.jpg" width="150" height="150"></a><center><span class="imagecaption">Amanda Askell</span></center></div></center>

<p></p><p>

</p></td><td>

<p></p><p>

 </p><center><div class="image" style=""><a href="https://www.seas.upenn.edu/~daphnei/" target="_blank"><img class="markdeep" src="images/daphneippolito.jpg" width="150" height="150"></a><center><span class="imagecaption">Daphne Ippolito</span></center></div></center>

<p></p><p>

</p></td></tr><tr valign="top"><td>

<p></p><p>

</p><center><div class="image" style=""><a href="https://scholar.google.com/citations?user=LWeVRdUAAAAJ&hl=en" target="_blank"><img class="markdeep" src="images/ethandyer.jpg" width="150" height="150"></a><center><span class="imagecaption">Ethan Dyer</span></center></div></center>

<p></p><p>

</p></td><td>

<p></p><p>

 </p><center><div class="image" style=""><a href="https://www.linkedin.com/in/guy-gur-ari" target="_blank"><img class="markdeep" src="images/guygurari.jpg" width="150" height="150"></a><center><span class="imagecaption">Guy Gur-Ari</span></center></div></center>

<p></p><p>

</p></td><td>

<p></p><p>

 </p><center><div class="image" style=""><a href="https://sites.krieger.jhu.edu/jared-kaplan/" target="_blank"><img class="markdeep" src="images/jaredkaplan.jpg" width="150" height="150"></a><center><span class="imagecaption">Jared Kaplan</span></center></div></center>

<p></p><p>

</p></td><td>

<p></p><p>

 </p><center><div class="image" style=""><a href="http://www.sohldickstein.com/" target="_blank"><img class="markdeep" src="images/jaschasohldickstein.jpg" width="150" height="150"></a><center><span class="imagecaption">Jascha Sohl-Dickstein</span></center></div></center>

<p></p><p>

</p></td></tr><tr valign="top"><td>

<p></p><p>

</p><center><div class="image" style=""><a href="https://katelee168.github.io/" target="_blank"><img class="markdeep" src="images/katherinelee.jpg" width="150" height="150"></a><center><span class="imagecaption">Katherine Lee</span></center></div></center>

<p></p><p>

</p></td><td>

<p></p><p>

 </p><center><div class="image" style=""><a href="https://www.linkedin.com/in/melanie-subbiah-7b702a8a" target="_blank"><img class="markdeep" src="images/melaniesubbiah.jpg" width="150" height="150"></a><center><span class="imagecaption">Melanie Subbiah</span></center></div></center>

<p></p><p>

</p></td><td>

<p></p><p>

 </p><center><div class="image" style=""><a href="http://vedantmisra.com/" target="_blank"><img class="markdeep" src="images/vedantmisra.jpg" width="150" height="150"></a><center><span class="imagecaption">Vedant Misra</span></center></div></center>

<p></p><p>

</p></td><td>

<p></p><p>

 </p><center><div class="image" style=""><a href="https://github.com/nottombrown" target="_blank"><img class="markdeep" src="images/tombrown.jpg" width="150" height="150"></a><center><span class="imagecaption">Tom Brown</span></center></div></center>

<p></p><p>

</p></td></tr><tr valign="top"><td>

<p></p><p>

</p><center><div class="image" style=""><a href="https://github.com/ajslone" target="_blank"><img class="markdeep" src="images/ambroseslone.jpg" width="150" height="150"></a><center><span class="imagecaption">Ambrose Slone</span></center></div></center>

<p></p><p>

</p></td><td>

<p></p><p>

 </p><center><div class="image" style=""><a href="https://acsweb.ucsd.edu/~wfedus/" target="_blank"><img class="markdeep" src="images/liamfedus.jpg" width="150" height="150"></a><center><span class="imagecaption">Liam Fedus</span></center></div></center>

<p></p><p>

</p></td><td>

<p></p><p>

 </p><center><div class="image" style=""><a href="http://berkeleysciencereview.com/author/dfreeman/" target="_blank"><img class="markdeep" src="images/danielfreeman.jpg" width="150" height="150"></a><center><span class="imagecaption">Daniel Freeman</span></center></div></center>

<p></p><p>

</p></td><td>

<p></p><p>

 </p><center><div class="image" style=""><a href="https://www.simonsfoundation.org/people/aitor-lewkowycz/" target="_blank"><img class="markdeep" src="images/aitorlewkowycz.jpg" width="150" height="150"></a><center><span class="imagecaption">Aitor Lewkowycz</span></center></div></center>

<p></p><p>

</p></td></tr></tbody></table>

</p>
<div class="nonumberh1">Benchmark developers</div>
<p>

Kristen Chiafullo,
Ethan Dyer,
Liam Fedus,
Noah Fiedel,
Daniel Freeman,
Guy Gur-Ari,
Jaehoon Lee,
Aitor Lewkowycz,
Gaurav Mishra,
Vedant Misra,
Isaac Noble,
Timothy Nguyen,
Danielle Perszyk,
Ambrose Slone,
Jascha Sohl-Dickstein

</p>
<div class="nonumberh1">Benchmark program committee</div>
<p>

Kyle Aitken,
Igor Babuschkin,
Adam Brown,
David Dohan,
Ethan Dyer,
Stanislav Fort,
Daniel Freeman,
Dar Gilboa,
Anna Golubeva,
Guy Gur-Ari,
Jesse Michael Han,
Boris Hanin,
Daniel Khashabi,
Aitor Lewkowycz,
Harsh Mehta,
Gaurav Mishra,
Timothy Nguyen,
Isaac Noble,
Alethea Power,
Ambrose Slone,
Jascha Sohl-Dickstein,
James Sully,
Neha Wadia

</p>
<div class="nonumberh1">Advisory committee</div>
<p>

<table width="100%"><tbody><tr valign="top"><td>

<p></p><p>

</p><center><div class="image" style=""><a href="https://cims.nyu.edu/~sbowman/" target="_blank"><img class="markdeep" src="images/sambowman.jpg" width="150" height="150"></a><center><span class="imagecaption">Samuel R. Bowman</span></center></div></center>

<p></p><p>

</p></td><td>

<p></p><p>

 </p><center><div class="image" style=""><a href="https://melaniemitchell.me/" target="_blank"><img class="markdeep" src="images/melaniemitchell.jpg" width="150" height="150"></a><center><span class="imagecaption">Melanie Mitchell</span></center></div></center>

<p></p><p>

</p></td><td>

<p></p><p>

 </p><center><div class="image" style=""><a href="https://cs.stanford.edu/~pliang/" target="_blank"><img class="markdeep" src="images/percyliang.jpg" width="150" height="150"></a><center><span class="imagecaption">Percy Liang</span></center></div></center>

<p></p><p>

</p></td><td>

<p></p><p>

 </p><center><div class="image" style=""><a href="https://yjernite.github.io/" target="_blank"><img class="markdeep" src="images/yacinejernite.jpg" width="150" height="150"></a><center><span class="imagecaption">Yacine Jernite</span></center></div></center>

<p></p><p>

</p></td></tr></tbody></table>

</p><p>

</p></span><div id="mdContextMenu" style="visibility:hidden"></div><div class="markdeepFooter"><i>formatted by <a href="https://casual-effects.com/markdeep" style="color:#999">Markdeep&nbsp;1.13&nbsp;&nbsp;</a></i><div style="display:inline-block;font-size:13px;font-family:'Times New Roman',serif;vertical-align:middle;transform:translate(-3px,-1px)rotate(135deg);">✒</div></div></body></html>
